{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  # VA Data tools\n",
    "  ## Tools for navigating VA Datasets\n",
    "  \n",
    "  ## History\n",
    "The VA open data portal (https://www.data.va.gov/) contains several thousand data files in Excel, CSV, and PDF formats.  Brent Brewington, using R, flattened out the dataset inventory from JSON to a CSV file.  Both the CSV file and the R code used to do this can be found at his repository:\n",
    "https://github.com/bbrewington/VA-open-data-mysandbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, requests, io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the latest version of Brent's CSV file and save it 'locally.' I only need to do this if something changes, really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VADataInventory = requests.get('https://raw.githubusercontent.com/bbrewington/VA-open-data-mysandbox/master/va_data_inventory_links_checked_20171202.csv')\n",
    "VADataInventory.raise_for_status()\n",
    "\n",
    "megadata = open('va_data_inventory.csv', 'wb')\n",
    "for chunk in VADataInventory.iter_content(100000):\n",
    "    megadata.write(chunk)\n",
    "    \n",
    "megadata.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!  When the file is downloaded, I'll just run from the following lines.  The use for these will become more clear as we dig through the various functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MegaData= open('va_data_inventory.csv', encoding = \"utf8\")\n",
    "megasheet = csv.reader(MegaData) # read this using the CSV module\n",
    "alldata = list(megasheet) # which is easier to work with in a list format\n",
    "headers = alldata[0]      # easily grab the headers this way\n",
    "file_extension_regex = re.compile(r'\\.((pdf|csv|xlsx?|zip|asp))', re.IGNORECASE) \n",
    "states = ['Alabama' ,'Alaska' ,'Arizona' ,'Arkansas' ,'California' ,'Colorado' ,'Connecticut' ,'Delaware' ,'Florida' ,'Georgia' ,'Hawaii' ,'Idaho' ,'Illinois' ,'Indiana' ,'Iowa' ,'Kansas' ,'Kentucky' ,'Louisiana' ,'Maine' ,'Maryland' ,'Massachusetts' ,'Michigan' ,'Minnesota' ,'Mississippi' ,'Missouri' ,'Montana' ,'Nebraska' ,'Nevada' ,'New Hampshire' ,'New Jersey' ,'New Mexico' ,'New York' ,'North Carolina' ,'North Dakota' ,'Ohio' ,'Oklahoma' ,'Oregon' ,'Pennsylvania' ,'Rhode' 'Island' ,'South Carolina' ,'South Dakota' ,'Tennessee' ,'Texas' ,'Utah' ,'Vermont' ,'Virginia' ,'Washington' ,'West' 'Virginia' ,'Wisconsin' ,'Wyoming']\n",
    "territories = ['American Samoa', 'Guam', 'Northern Mariana Islands', 'Puerto Rico']\n",
    "states_territories = states + territories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! First, note that Brent's CSV file contains the following headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without losing too much by 'overcleaning' the CSV file, these tools will help us browse through it. Remember that we are now treating this CSV file in list format. So each row is a list, and each data element is an element in the list matching the headers.  i.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools will help us sort through all this without 'overcleaning' this datafile. The first two are just a basic search.\n",
    "You can search a line or the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsearch(line, term): # search for any term in a line/dataset\n",
    "    search_term = re.compile(r'.*(%s).*' % (term), re.IGNORECASE )\n",
    "    fields = filter(search_term.match, alldata[line])\n",
    "    return(list(fields))\n",
    "\n",
    "def searchall(term): # search entire CSV file. The output will tell you what line it's on.\n",
    "    searchresults = []  \n",
    "    for line in range(0,len(alldata)):\n",
    "        if not vsearch(line, term) == []:\n",
    "            searchresults = searchresults + [vsearch(line, term), 'line:' + str(line -1)]\n",
    "    return(searchresults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsearch(1, 'state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted all the 'by state' datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchall('by state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line is given, so I can pull up that dataset.  468 looks interesting.  This is a little hard to look at though.  A few more tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(line): # What keywords descibe this file?\n",
    "    return(alldata[line][7])\n",
    "\n",
    "def whatformat(line):# This will tell you what format a datafile is in\n",
    "    return(alldata[line][31])\n",
    "\n",
    "\n",
    "def getheaders(line): # get headers of csv or excel file\n",
    "    earl = whaturl(line) # assuming the headers are on the first line\n",
    "    print('Getting file...') \n",
    "    # for excel files\n",
    "    if whatformat(line) in ('xlsx', 'xls'): \n",
    "        print('Excel file. Getting headers...')\n",
    "        df = pd.read_excel(earl)\n",
    "        return(df.columns.tolist())\n",
    "    # for CSV files\n",
    "    elif whatformat(line) == 'csv':\n",
    "        print('CSV file. Getting headers...')\n",
    "        res = requests.get(whaturl(line))\n",
    "        df = pd.read_csv(io.StringIO(res.text))\n",
    "        return(df.columns.tolist())\n",
    "    else:\n",
    "        print('Oh dear! This does not appear to be an excel or CSV file.')\n",
    "        \n",
    "def whaturl(line): # Where can I find the file for download\n",
    "    return(alldata[line][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords(468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatformat(468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whaturl(468)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will pull the headers down into a pandas dataframe.  This one could use some help. It pulls  down the entire file first to give you the headers.  There are a number of reasons why this might not always work, including the assumption that the headers are always on the first row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getheaders(468)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, like that.  How about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getheaders(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better.  Want to pull the entire dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAPandas(line): # Pull down CSV or Excel files into Pandas Dataframe\n",
    "    earl = whaturl(line) # assuming the headers are on the first line\n",
    "    print('Getting file...') \n",
    "    # for excel files\n",
    "    if whatformat(line) in ('xlsx', 'xls'): \n",
    "        print('Excel file. Retrieving into Pandas Dataframe...')\n",
    "        df = pd.read_excel(earl)\n",
    "        return(df)\n",
    "    # for CSV files\n",
    "    elif whatformat(line) == 'csv':\n",
    "        print('CSV file. Retrieving into Pandas Dataframe...')\n",
    "        res = requests.get(whaturl(line))\n",
    "        df = pd.read_csv(io.StringIO(res.text))\n",
    "        return(df)\n",
    "    else:\n",
    "        print('Oh dear! This does not appear to be an excel or CSV file.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting file...\n",
      "Excel file. Retrieving into Pandas Dataframe...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Date Gender                                                POS  \\\n",
       " 0    2015-09-30      F                          (a)\\nAll Veterans\\n (b+c)   \n",
       " 1    2015-09-30      F    (b)\\nWartime Veterans\\n (i+j+k+m+n+p+q+r+t+u+v)   \n",
       " 2    2015-09-30      F             (c )\\nPeacetime Veterans\\n (h+l+o+s+w)   \n",
       " 3    2015-09-30      F                               (d)\\n WWII\\n (i+j+k)   \n",
       " 4    2015-09-30      F                   (e)\\nKorean Conflict \\n(j+k+m+n)   \n",
       " 5    2015-09-30      F                      (f)\\nVietnam Era\\n(k+n+p+q+r)   \n",
       " 6    2015-09-30      F                     (g)\\nGulf War Era\\n(q+r+t+u+v)   \n",
       " 7    2015-09-30      F                                      (h)\\nPre-WWII   \n",
       " 8    2015-09-30      F                                   (i)\\nWWII\\n only   \n",
       " 9    2015-09-30      F                              (j)\\nWWII &\\n KC only   \n",
       " 10   2015-09-30      F                                (k)\\nWWII, KC,\\nVNE   \n",
       " 11   2015-09-30      F                          (l)\\n Between\\n WWII & KC   \n",
       " 12   2015-09-30      F                                     (m)\\nKC\\n only   \n",
       " 13   2015-09-30      F                               (n)\\nKC &\\n VNE only   \n",
       " 14   2015-09-30      F                            (o)\\nBetween KC &\\n VNE   \n",
       " 15   2015-09-30      F                                    (p)\\nVNE\\n only   \n",
       " 16   2015-09-30      F                (q)\\nVNE & GW Era –\\n Pre 9/11 only   \n",
       " 17   2015-09-30      F    (r)\\nVNE, GW Era – Pre 9/11,\\n GW Era – Post...   \n",
       " 18   2015-09-30      F                         (s)\\nBetween VNE &\\nGW Era   \n",
       " 19   2015-09-30      F                      (t)\\nGW Era – \\nPre 9/11 only   \n",
       " 20   2015-09-30      F    (u)\\nGW Era – Pre 9/11 &\\nGW Era – Post 9/11...   \n",
       " 21   2015-09-30      F                     (v)\\nGW Era – \\nPost 9/11 only   \n",
       " 22   2015-09-30      F                                 (w)\\nPost-GW\\n Era   \n",
       " 23   2015-09-30      M                          (a)\\nAll Veterans\\n (b+c)   \n",
       " 24   2015-09-30      M    (b)\\nWartime Veterans\\n (i+j+k+m+n+p+q+r+t+u+v)   \n",
       " 25   2015-09-30      M             (c )\\nPeacetime Veterans\\n (h+l+o+s+w)   \n",
       " 26   2015-09-30      M                               (d)\\n WWII\\n (i+j+k)   \n",
       " 27   2015-09-30      M                   (e)\\nKorean Conflict \\n(j+k+m+n)   \n",
       " 28   2015-09-30      M                      (f)\\nVietnam Era\\n(k+n+p+q+r)   \n",
       " 29   2015-09-30      M                     (g)\\nGulf War Era\\n(q+r+t+u+v)   \n",
       " ...         ...    ...                                                ...   \n",
       " 1396 2045-09-30      F                (q)\\nVNE & GW Era –\\n Pre 9/11 only   \n",
       " 1397 2045-09-30      F    (r)\\nVNE, GW Era – Pre 9/11,\\n GW Era – Post...   \n",
       " 1398 2045-09-30      F                         (s)\\nBetween VNE &\\nGW Era   \n",
       " 1399 2045-09-30      F                      (t)\\nGW Era – \\nPre 9/11 only   \n",
       " 1400 2045-09-30      F    (u)\\nGW Era – Pre 9/11 &\\nGW Era – Post 9/11...   \n",
       " 1401 2045-09-30      F                     (v)\\nGW Era – \\nPost 9/11 only   \n",
       " 1402 2045-09-30      F                                 (w)\\nPost-GW\\n Era   \n",
       " 1403 2045-09-30      M                          (a)\\nAll Veterans\\n (b+c)   \n",
       " 1404 2045-09-30      M    (b)\\nWartime Veterans\\n (i+j+k+m+n+p+q+r+t+u+v)   \n",
       " 1405 2045-09-30      M             (c )\\nPeacetime Veterans\\n (h+l+o+s+w)   \n",
       " 1406 2045-09-30      M                               (d)\\n WWII\\n (i+j+k)   \n",
       " 1407 2045-09-30      M                   (e)\\nKorean Conflict \\n(j+k+m+n)   \n",
       " 1408 2045-09-30      M                      (f)\\nVietnam Era\\n(k+n+p+q+r)   \n",
       " 1409 2045-09-30      M                     (g)\\nGulf War Era\\n(q+r+t+u+v)   \n",
       " 1410 2045-09-30      M                                      (h)\\nPre-WWII   \n",
       " 1411 2045-09-30      M                                   (i)\\nWWII\\n only   \n",
       " 1412 2045-09-30      M                              (j)\\nWWII &\\n KC only   \n",
       " 1413 2045-09-30      M                                (k)\\nWWII, KC,\\nVNE   \n",
       " 1414 2045-09-30      M                          (l)\\n Between\\n WWII & KC   \n",
       " 1415 2045-09-30      M                                     (m)\\nKC\\n only   \n",
       " 1416 2045-09-30      M                               (n)\\nKC &\\n VNE only   \n",
       " 1417 2045-09-30      M                            (o)\\nBetween KC &\\n VNE   \n",
       " 1418 2045-09-30      M                                    (p)\\nVNE\\n only   \n",
       " 1419 2045-09-30      M                (q)\\nVNE & GW Era –\\n Pre 9/11 only   \n",
       " 1420 2045-09-30      M    (r)\\nVNE, GW Era – Pre 9/11,\\n GW Era – Post...   \n",
       " 1421 2045-09-30      M                         (s)\\nBetween VNE &\\nGW Era   \n",
       " 1422 2045-09-30      M                      (t)\\nGW Era – \\nPre 9/11 only   \n",
       " 1423 2045-09-30      M    (u)\\nGW Era – Pre 9/11 &\\nGW Era – Post 9/11...   \n",
       " 1424 2045-09-30      M                     (v)\\nGW Era – \\nPost 9/11 only   \n",
       " 1425 2045-09-30      M                                 (w)\\nPost-GW\\n Era   \n",
       " \n",
       "           Veterans  \n",
       " 0     1.835849e+06  \n",
       " 1     1.405461e+06  \n",
       " 2     4.303885e+05  \n",
       " 3     4.301124e+04  \n",
       " 4     5.097834e+04  \n",
       " 5     2.406002e+05  \n",
       " 6     1.098102e+06  \n",
       " 7     8.309542e+02  \n",
       " 8     4.156957e+04  \n",
       " 9     1.202997e+03  \n",
       " 10    2.386695e+02  \n",
       " 11    3.417177e+03  \n",
       " 12    4.647991e+04  \n",
       " 13    3.056758e+03  \n",
       " 14    5.024125e+04  \n",
       " 15    2.148114e+05  \n",
       " 16    1.903989e+04  \n",
       " 17    3.453539e+03  \n",
       " 18    3.758991e+05  \n",
       " 19    4.042195e+05  \n",
       " 20    1.931501e+05  \n",
       " 21    4.782386e+05  \n",
       " 22    0.000000e+00  \n",
       " 23    1.894771e+07  \n",
       " 24    1.458250e+07  \n",
       " 25    4.365203e+06  \n",
       " 26    8.963206e+05  \n",
       " 27    1.752564e+06  \n",
       " 28    6.772586e+06  \n",
       " 29    5.783795e+06  \n",
       " ...            ...  \n",
       " 1396  3.723274e+03  \n",
       " 1397  6.986183e+02  \n",
       " 1398  1.866493e+05  \n",
       " 1399  2.916737e+05  \n",
       " 1400  1.759602e+05  \n",
       " 1401  8.185752e+05  \n",
       " 1402  6.971605e+05  \n",
       " 1403  9.786184e+06  \n",
       " 1404  6.164919e+06  \n",
       " 1405  3.621266e+06  \n",
       " 1406  1.984164e-01  \n",
       " 1407  1.687833e+02  \n",
       " 1408  4.916843e+05  \n",
       " 1409  5.715155e+06  \n",
       " 1410  4.221026e-05  \n",
       " 1411  1.595135e-01  \n",
       " 1412  2.539572e-02  \n",
       " 1413  1.350716e-02  \n",
       " 1414  7.920972e-01  \n",
       " 1415  1.455786e+02  \n",
       " 1416  2.316582e+01  \n",
       " 1417  4.917835e+03  \n",
       " 1418  4.495949e+05  \n",
       " 1419  3.371225e+04  \n",
       " 1420  8.353972e+03  \n",
       " 1421  1.100646e+06  \n",
       " 1422  1.406500e+06  \n",
       " 1423  9.048036e+05  \n",
       " 1424  3.361785e+06  \n",
       " 1425  2.515701e+06  \n",
       " \n",
       " [1426 rows x 4 columns], int)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAPandas(6), type(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also to be improved is the following function, which is supposed to tell you what, if any state or territory data is included in a particular header.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatstate(line):\n",
    "    which_states = []\n",
    "    for x in alldata[line]:\n",
    "        for state in states_territories:\n",
    "            if state.lower() in x.lower():\n",
    "                which_states = which_states + [state]\n",
    "    if len(which_states) == 0:\n",
    "        return(\"No state names found\")\n",
    "    else:\n",
    "        return(list(set(which_states))) #turned into a set to remove dups, then back into list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example. What states is covered in the first, third and seventh dataset? Note that we do include terrotories, and will have ways for sorting out territories and states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Delaware'], ['Florida'], 'No state names found')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatstate(564), whatstate(565), whatstate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what files have state data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statedata = [] \n",
    "for i in range(0,len(alldata)):\n",
    "    if not whatstate(i) == 'No state names found':\n",
    "        statedata = statedata + [[whatstate(i)[0], i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(statedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other projects and future uses:\n",
    "* Find all files with the same or similar headers\n",
    "* Find data per county?\n",
    "* Make the states/territories dataset into a dictionary format using state abbreviations\n",
    "* Brent's CSV file is great, but maybe we could pull from the JSON directly?\n",
    "* Fix whatever newbie coding mistakes I have made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
